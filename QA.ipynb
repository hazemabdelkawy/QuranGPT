{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hazemabdelkawy/miniforge3/envs/ml_env/lib/python3.8/site-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import json  # Import the JSON module for working with JSON data\n",
    "import numpy as np  # Import NumPy, a library for working with arrays and matrices\n",
    "import uuid  # Import the UUID module for generating UUIDs (Universally Unique Identifiers)\n",
    "import pinecone  # Import the Pinecone library for working with vector similarity search\n",
    "from typing import List, Tuple  # Import List and Tuple types for type hinting\n",
    "import src.config as config  # Import a module called \"config\" from a subdirectory called \"src\"\n",
    "import logging  # Import the logging module for logging messages\n",
    "import time  # Import the time module for measuring time\n",
    "import openai  # Import the OpenAI library for natural language processing\n",
    "from lingua import Language, LanguageDetectorBuilder  # Import the Lingua library for language detection and identification\n",
    "\n",
    "import os  # Import the os library for setting environment variables\n",
    "from langchain.llms import OpenAI  # Import the OpenAI module from the langchain package for interacting with the OpenAI GPT-3 model\n",
    "from langchain.prompts import PromptTemplate  # Import the PromptTemplate module from the langchain package for generating prompts for the OpenAI GPT-3 model\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = config.OPENAI_KEY # Set the OPENAI_API_KEY environment variable to the OpenAI API key stored in the config module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, data_file: str):\n",
    "        \"\"\"\n",
    "        A class for loading and processing data from a JSON file.\n",
    "\n",
    "        Args:\n",
    "        - data_file (str): The path to the JSON file to load.\n",
    "        \"\"\"\n",
    "        self.data_file = data_file\n",
    "        self.quran_data = None  # Initialize Quran data to None\n",
    "        self.embeddings = []  # Initialize an empty list for embeddings\n",
    "        self.metadata = []  # Initialize an empty list for metadata\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load Quran data from a JSON file and store it in the `quran_data` attribute.\n",
    "        \"\"\"\n",
    "        with open(self.data_file, \"r\") as f:\n",
    "            self.quran_data = json.load(f)\n",
    "\n",
    "    def process_data(self):\n",
    "        \"\"\"\n",
    "        Process the Quran data by extracting embeddings and metadata for each verse.\n",
    "        \"\"\"\n",
    "        for verse in self.quran_data:\n",
    "            self.embeddings.append(verse.get(\"embedding\"))  # Add the verse's embedding to the embeddings list\n",
    "            self.metadata.append(self.encode_metadata_in_id({\n",
    "                \"surah\": verse.get(\"surah\"),\n",
    "                \"ayah\": verse.get(\"ayah\"),\n",
    "                \"text_ar\": verse.get(\"text_ar\"),\n",
    "                \"text_en\": verse.get(\"text_en\")\n",
    "            }))  # Add the verse's metadata, encoded in JSON format, to the metadata list\n",
    "\n",
    "    @staticmethod\n",
    "    def encode_metadata_in_id(metadata: dict) -> str:\n",
    "        \"\"\"\n",
    "        Encode metadata in JSON format and return it as a string.\n",
    "\n",
    "        Args:\n",
    "        - metadata (dict): A dictionary containing metadata for a verse.\n",
    "\n",
    "        Returns:\n",
    "        - str: The encoded metadata in JSON format.\n",
    "        \"\"\"\n",
    "        return json.dumps(metadata, ensure_ascii=False, indent=4)\n",
    "\n",
    "    @staticmethod\n",
    "    def decode_metadata_from_id(item_id: str) -> dict:\n",
    "        \"\"\"\n",
    "        Decode metadata from a JSON string and return it as a dictionary.\n",
    "\n",
    "        Args:\n",
    "        - item_id (str): A string containing metadata in JSON format.\n",
    "\n",
    "        Returns:\n",
    "        - dict: The decoded metadata as a dictionary.\n",
    "        \"\"\"\n",
    "        return json.loads(item_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PineconeManager:\n",
    "    def __init__(self, api_key: str, index_name: str, dimension: int, create_index: bool = False):\n",
    "        \"\"\"\n",
    "        A class for managing a Pinecone index.\n",
    "\n",
    "        Args:\n",
    "        - api_key (str): The Pinecone API key to use.\n",
    "        - index_name (str): The name of the Pinecone index to manage.\n",
    "        - dimension (int): The dimensionality of the vectors to be indexed.\n",
    "        - create_index (bool): Whether to create a new index with the specified name and dimensionality. Defaults to False.\n",
    "        \"\"\"\n",
    "        self.api_key = api_key\n",
    "        self.index_name = index_name\n",
    "        self.dimension = dimension\n",
    "        pinecone.init(api_key=self.api_key)  # Initialize the Pinecone API\n",
    "        if create_index:\n",
    "            pinecone.create_index(name=self.index_name, metric=\"cosine\", dimension=self.dimension)  # Create a new index with the specified name and dimensionality\n",
    "        self.index = pinecone.Index(index_name=self.index_name)  # Initialize the index object\n",
    "        self.metadata_storage = {}  # Initialize an empty dictionary for storing metadata\n",
    "\n",
    "    def index_data(self, metadata, embeddings):\n",
    "        \"\"\"\n",
    "        Index data by adding metadata and embeddings to the Pinecone index.\n",
    "\n",
    "        Args:\n",
    "        - metadata (list): A list of metadata dictionaries, where each dictionary corresponds to a single vector.\n",
    "        - embeddings (list): A list of embeddings, where each embedding corresponds to a single vector.\n",
    "        \"\"\"\n",
    "        for verse_meta, embedding in zip(metadata, embeddings):\n",
    "            print(f\"Uploading : {verse_meta}\")  # Print a message indicating that the metadata is being uploaded\n",
    "            short_id = self.generate_short_id()  # Generate a short ID for the metadata\n",
    "            self.metadata_storage[short_id] = verse_meta  # Add the metadata to the metadata storage dictionary\n",
    "            self.index.upsert([(short_id, embedding)])  # Add the metadata and embedding to the Pinecone index\n",
    "\n",
    "    def query_index(self, vector, top_k=20):\n",
    "        \"\"\"\n",
    "        Query the Pinecone index with a vector and retrieve the top K most similar vectors.\n",
    "\n",
    "        Args:\n",
    "        - vector (list): A list representing a query vector.\n",
    "        - top_k (int): The number of results to return. Defaults to 20.\n",
    "\n",
    "        Returns:\n",
    "        - list: A list of tuples, where each tuple contains a short ID and a similarity score.\n",
    "        \"\"\"\n",
    "        search_results = self.index.query(vector=vector, top_k=top_k)  # Query the Pinecone index with the vector\n",
    "        return search_results\n",
    "\n",
    "    def save_metadata_storage(self, file_name: str):\n",
    "        \"\"\"\n",
    "        Save the metadata storage dictionary to a file in JSON format.\n",
    "\n",
    "        Args:\n",
    "        - file_name (str): The name of the file to save the metadata to.\n",
    "        \"\"\"\n",
    "        with open(file_name, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.metadata_storage, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    def load_metadata_storage(self, file_name: str) -> dict:\n",
    "        \"\"\"\n",
    "        Load the metadata storage dictionary from a file in JSON format.\n",
    "\n",
    "        Args:\n",
    "        - file_name (str): The name of the file to load the metadata from.\n",
    "\n",
    "        Returns:\n",
    "        - dict: The metadata storage dictionary.\n",
    "        \"\"\"\n",
    "        with open(file_name, 'r', encoding='utf-8') as f:\n",
    "            metadata_storage = json.load(f)\n",
    "        self.metadata_storage = metadata_storage\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_short_id():\n",
    "        \"\"\"\n",
    "        Generate a short, random ID.\n",
    "\n",
    "        Returns:\n",
    "        - str: The generated ID.\n",
    "        \"\"\"\n",
    "        return str(uuid.uuid4())[:8]  # Generate a random UUID and return the first 8 characters as the short ID.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIManager:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the OpenAIManager class with pre-defined languages, language detector,\n",
    "        OpenAI model, and a prompt template for translation.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.languages = [Language.ENGLISH, Language.ARABIC]\n",
    "        self.lang_detector = LanguageDetectorBuilder.from_languages(*self.languages).build()\n",
    "        self.openai_llm = OpenAI(temperature=0)\n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"query\"],\n",
    "            template=\"Translate the following arabic text into english : {query}\"\n",
    "        )\n",
    "        \n",
    "    def gpt3_embedding(self, content: str, engine: str = 'text-embedding-ada-002') -> list:\n",
    "        \"\"\"\n",
    "        Generates an embedding for the input content using OpenAI GPT-3.\n",
    "\n",
    "        Args:\n",
    "            content (str): The input content for which the embedding is required.\n",
    "            engine (str, optional): The engine to be used for generating the embedding. Defaults to 'text-embedding-ada-002'.\n",
    "\n",
    "        Returns:\n",
    "            list: The generated embedding as a list of floats.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = openai.Embedding.create(input=content, engine=engine)\n",
    "            vector = response['data'][0]['embedding']\n",
    "            return vector\n",
    "        except Exception as e:\n",
    "            logging.error(f'Embedding failed. Error message: {e}')\n",
    "\n",
    "    def extract_embedding(self, text: str) -> list:\n",
    "        \"\"\"\n",
    "        Extracts the embedding for the given text using GPT-3.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text for which the embedding is required.\n",
    "\n",
    "        Returns:\n",
    "            list: The extracted embedding as a list of floats.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            embedding = self.gpt3_embedding(text)\n",
    "        except:\n",
    "            while True:\n",
    "                try:\n",
    "                    if len(text) > 8191:\n",
    "                        logging.warning('[OPENAI ERROR] Trying to get shorter input < 8191 for text...')\n",
    "                        embedding = self.gpt3_embedding(text[:8191])\n",
    "                    else:\n",
    "                        embedding = self.gpt3_embedding(text)\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    logging.error(f'Trying to get the embedding for text. Error message: {e}')\n",
    "                    time.sleep(5)\n",
    "        return embedding\n",
    "    \n",
    "    def translate(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Translates the input Arabic text to English using the OpenAI model.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input Arabic text to be translated.\n",
    "\n",
    "        Returns:\n",
    "            str: The translated English text.\n",
    "        \"\"\"\n",
    "        \n",
    "        translated_text = self.openai_llm(self.prompt.format(query=text))\n",
    "        return translated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 8348fc0d, Similarity: 0.847318172, Metadata: {\n",
      "    \"surah\": 7,\n",
      "    \"ayah\": 196,\n",
      "    \"text_ar\": \"إن وليي الله الذي نزل الكتاب وهو يتولى الصالحين\",\n",
      "    \"text_en\": \"Verily my protector is Allah who hath revealed the Book, and He protecteth the righteous.\"\n",
      "}\n",
      "ID: b517106a, Similarity: 0.831350386, Metadata: {\n",
      "    \"surah\": 48,\n",
      "    \"ayah\": 3,\n",
      "    \"text_ar\": \"وينصرك الله نصرا عزيزا\",\n",
      "    \"text_en\": \"And that Allah may succour thee with a mighty succour.\"\n",
      "}\n",
      "ID: b4722a9c, Similarity: 0.827713251, Metadata: {\n",
      "    \"surah\": 44,\n",
      "    \"ayah\": 42,\n",
      "    \"text_ar\": \"إلا من رحم الله إنه هو العزيز الرحيم\",\n",
      "    \"text_en\": \"Save those on whom Allah will have mercy. Verily He! He is the Mighty, the Merciful.\"\n",
      "}\n",
      "ID: ec4b6d0e, Similarity: 0.827417135, Metadata: {\n",
      "    \"surah\": 16,\n",
      "    \"ayah\": 81,\n",
      "    \"text_ar\": \"والله جعل لكم مما خلق ظلالا وجعل لكم من الجبال أكنانا وجعل لكم سرابيل تقيكم الحر وسرابيل تقيكم بأسكم كذلك يتم نعمته عليكم لعلكم تسلمون\",\n",
      "    \"text_en\": \"And Allah hath appointed for you, of that which He hath created shades, and He hath appointed for you from the mountains places of retreat, and He hath appointed for you coats protecting you from the heat and coats protecting you from the violence. Thus He perfecteth His favour on you that haply ye may submit.\"\n",
      "}\n",
      "ID: e7c4e1ed, Similarity: 0.822581828, Metadata: {\n",
      "    \"surah\": 48,\n",
      "    \"ayah\": 2,\n",
      "    \"text_ar\": \"ليغفر لك الله ما تقدم من ذنبك وما تأخر ويتم نعمته عليك ويهديك صراطا مستقيما\",\n",
      "    \"text_en\": \"That Allah may forgive thee that which hath preceded of thy fault and that which may come later, and may accomplish the more His favour on thee, and may keep thee guided on the straight path.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Load data from the specified file and process it\n",
    "    data_loader = DataLoader(\"./data/quran_GPT_embeddings_.json\")\n",
    "    data_loader.load_data()\n",
    "    data_loader.process_data()\n",
    "\n",
    "    # Initialize Pinecone manager with API key, index name, and dimension\n",
    "    pinecone_api_key = config.PINECONE_KEY\n",
    "    index_name = \"gpt\"\n",
    "    dimension = 1536\n",
    "\n",
    "    # Create an instance of PineconeManager and set create_index to True if you want to create a new index\n",
    "    pinecone_manager = PineconeManager(pinecone_api_key, index_name, dimension, create_index=False)\n",
    "    \n",
    "    # Index the data and save metadata to storage (uncomment the following lines if needed)\n",
    "    # pinecone_manager.index_data(data_loader.metadata, data_loader.embeddings)\n",
    "    # pinecone_manager.save_metadata_storage(\"metadata_storage.json\")\n",
    "\n",
    "    # Load metadata storage from the specified file\n",
    "    pinecone_manager.load_metadata_storage(\"data/metadata_storage.json\")\n",
    "\n",
    "    # Initialize OpenAI manager\n",
    "    openai_manager = OpenAIManager()\n",
    "    \n",
    "    # Get user input and detect its language; if it's Arabic, translate it to English\n",
    "    input_text  = input(\"Enter your query: \")\n",
    "    if openai_manager.lang_detector.detect_language_of(input_text) == Language.ARABIC:\n",
    "        input_text = openai_manager.translate(input_text)\n",
    "        \n",
    "    # Extract the embedding for the input text\n",
    "    input_query_embedding = openai_manager.extract_embedding(input_text)\n",
    "\n",
    "    # Query the Pinecone index using the extracted embedding and retrieve the top 5 results\n",
    "    search_results = pinecone_manager.query_index(input_query_embedding, top_k=5)\n",
    "    for item in search_results['matches']:\n",
    "        item_id = item['id']\n",
    "        item_metadata = pinecone_manager.metadata_storage[item_id]\n",
    "        similarity = item['score']\n",
    "        print(f\"ID: {item_id}, Similarity: {similarity}, Metadata: {item_metadata}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
