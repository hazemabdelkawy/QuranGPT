{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import plotly.graph_objs as go\n",
    "import torch\n",
    "\n",
    "import openai\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import src.config as config\n",
    "\n",
    "openai.api_key = config.OPENAI_API_KEY\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surah_names = {\n",
    "    '1': 'Al-Fatihah',\n",
    "    '2': 'Al-Baqarah',\n",
    "    '3': 'Aal-E-Imran',\n",
    "    '4': 'An-Nisa',\n",
    "    '5': 'Al-Maidah',\n",
    "    '6': 'Al-An\\'am',\n",
    "    '7': 'Al-A\\'raf',\n",
    "    '8': 'Al-Anfal',\n",
    "    '9': 'At-Tawbah',\n",
    "    '10': 'Yunus',\n",
    "    '11': 'Hud',\n",
    "    '12': 'Yusuf',\n",
    "    '13': 'Ar-Ra\\'d',\n",
    "    '14': 'Ibrahim',\n",
    "    '15': 'Al-Hijr',\n",
    "    '16': 'An-Nahl',\n",
    "    '17': 'Al-Isra',\n",
    "    '18': 'Al-Kahf',\n",
    "    '19': 'Maryam',\n",
    "    '20': 'Taha',\n",
    "    '21': 'Al-Anbiya',\n",
    "    '22': 'Al-Hajj',\n",
    "    '23': 'Al-Mu\\'minun',\n",
    "    '24': 'An-Nur',\n",
    "    '25': 'Al-Furqan',\n",
    "    '26': 'Ash-Shu\\'ara',\n",
    "    '27': 'An-Naml',\n",
    "    '28': 'Al-Qasas',\n",
    "    '29': 'Al-Ankabut',\n",
    "    '30': 'Ar-Rum',\n",
    "    '31': 'Luqman',\n",
    "    '32': 'As-Sajda',\n",
    "    '33': 'Al-Ahzab',\n",
    "    '34': 'Saba',\n",
    "    '35': 'Fatir',\n",
    "    '36': 'Ya-Sin',\n",
    "    '37': 'As-Saffat',\n",
    "    '38': 'Sad',\n",
    "    '39': 'Az-Zumar',\n",
    "    '40': 'Ghafir',\n",
    "    '41': 'Fussilat',\n",
    "    '42': 'Ash-Shura',\n",
    "    '43': 'Az-Zukhruf',\n",
    "    '44': 'Ad-Dukhan',\n",
    "    '45': 'Al-Jathiya',\n",
    "    '46': 'Al-Ahqaf',\n",
    "    '47': 'Muhammad',\n",
    "    '48': 'Al-Fath',\n",
    "    '49': 'Al-Hujraat',\n",
    "    '50': 'Qaf',\n",
    "    '51': 'Adh-Dhariyat',\n",
    "    '52': 'At-Tur',\n",
    "    '53': 'An-Najm',\n",
    "    '54': 'Al-Qamar',\n",
    "    '55': 'Ar-Rahman',\n",
    "    '56': 'Al-Waqi\\'a',\n",
    "    '57': 'Al-Hadid',\n",
    "    '58': 'Al-Mujadila',\n",
    "    '59': 'Al-Hashr',\n",
    "    '60': 'Al-Mumtahina',\n",
    "    '61': 'As-Saff',\n",
    "    '62': 'Al-Jumu\\'a',\n",
    "    '63': 'Al-Munafiqun',\n",
    "    '64': 'At-Taghabun',\n",
    "    '65': 'At-Talaq',\n",
    "    '66': 'At-Tahrim',\n",
    "    '67': 'Al-Mulk',\n",
    "    '68': 'Al-Qalam',\n",
    "    '69': 'Al-Haaqqa',\n",
    "    '70': 'Al-Ma\\'arij',\n",
    "    '71': 'Nuh',\n",
    "    '72': 'Al-Jinn',\n",
    "    '73': 'Al-Muzzammil',\n",
    "    '74': 'Al-Muddaththir',\n",
    "    '75': 'Al-Qiyama',\n",
    "    '76': 'Al-Insan',\n",
    "    '77': 'Al-Mursalat',\n",
    "    '78': 'An-Naba',\n",
    "    '79': 'An-Nazi\\'at',\n",
    "    '80': 'Abasa',\n",
    "    '81': 'At-Takwir',\n",
    "    '82': 'Al-Infitar',\n",
    "    '83': 'Al-Mutaffifin',\n",
    "    '84': 'Al-Inshiqaq',\n",
    "    '85': 'Al-Buruj',\n",
    "    '86': 'At-Tariq',\n",
    "    '87': 'Al-A\\'la',\n",
    "    '88': 'Al-Ghashiya',\n",
    "    '89': 'Al-Fajr',\n",
    "    '90': 'Al-Balad',\n",
    "    '91': 'Ash-Shams',\n",
    "    '92': 'Al-Lail',\n",
    "    '93': 'Ad-Duha',\n",
    "    '94': 'Al-Inshirah',\n",
    "    '95': 'At-Tin',\n",
    "    '96': 'Al-Alaq',\n",
    "    '97': 'Al-Qadr',\n",
    "    '98': 'Al-Bayyina',\n",
    "    '99': 'Az-Zalzala',\n",
    "    '100': 'Al-Adiyat',\n",
    "    '101': 'Al-Qari\\'a',\n",
    "    '102': 'At-Takathur',\n",
    "    '103': 'Al-Asr',\n",
    "    '104': 'Al-Humaza',\n",
    "    '105': 'Al-Fil',\n",
    "    '106': 'Quraysh',\n",
    "    '107': 'Al-Ma\\'un',\n",
    "    '108': 'Al-Kawthar',\n",
    "    '109': 'Al-Kafirun',\n",
    "    '110': 'An-Nasr',\n",
    "    '111': 'Al-Masad',\n",
    "    '112': 'Al-Ikhlas',\n",
    "    '113': 'Al-Falaq',\n",
    "    '114': 'An-Nas'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Read the Quran and its translation from text files\n",
    "\n",
    "with open('data/quran_en.txt', 'r', encoding='utf-8') as f:\n",
    "    quran_en = f.readlines()\n",
    "quran_en = [verse.strip().split('|') for verse in quran_en]\n",
    "\n",
    "with open('data/quran_ar.txt', 'r', encoding='utf-8') as f:\n",
    "    quran_ar = f.readlines()\n",
    "    \n",
    "quran_ar_new = []\n",
    "for verse in quran_ar:\n",
    "    surah, ayah, text = verse.strip().split('|')\n",
    "    if text.startswith('بسم الله الرحمن الرحيم'):\n",
    "        bismillah = 'بسم الله الرحمن الرحيم'\n",
    "        verse = text.replace(bismillah, '').strip()\n",
    "        if len(verse) > 1:\n",
    "            quran_ar_new.append([surah, ayah, verse])\n",
    "        else :\n",
    "            quran_ar_new.append([surah, ayah, bismillah])\n",
    "    else:\n",
    "        quran_ar_new.append([surah, ayah, text])\n",
    "\n",
    "# Merge Arabic and English Quran data\n",
    "quran_data = []\n",
    "for i in range(len(quran_en)):\n",
    "    quran_data.append({\n",
    "        'surah': int(quran_en[i][0]),\n",
    "        'ayah': int(quran_en[i][1]),\n",
    "        'text_en': quran_en[i][2],\n",
    "        'text_ar': quran_ar_new[i][2]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(quran_data[0])\n",
    "print(quran_data[6235])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def export_embeddings_to_json(embeddings: list, file_path: str):\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(embeddings, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def visualize_embeddings_from_json(json_file):\n",
    "    with open(json_file, 'r') as file:\n",
    "        quran_json = json.load(file)\n",
    "    \n",
    "    # Extract embeddings from JSON and convert them to numpy arrays\n",
    "    embeddings = [np.array(item['embedding']) for item in quran_json]\n",
    "\n",
    "    # Reduce dimensionality of embeddings using t-SNE\n",
    "    perplexity = min(len(embeddings) - 1, 30)  # Use either 30 or n_samples - 1, whichever is smaller\n",
    "    tsne = TSNE(n_components=3, random_state=7)\n",
    "    embeddings_3d = tsne.fit_transform(np.array(embeddings))\n",
    "\n",
    "    # Create a dictionary to map each surah to a unique color\n",
    "    colors = {}\n",
    "    for surah in pd.DataFrame(quran_json)['surah'].unique():\n",
    "        colors[surah] = np.random.choice(range(256), size=3)\n",
    "\n",
    "    # Create scatter plot\n",
    "    fig = go.Figure()\n",
    "    for surah in pd.DataFrame(quran_json)['surah'].unique():\n",
    "        data = pd.DataFrame(quran_json)[pd.DataFrame(quran_json)['surah'] == surah]\n",
    "        color = f'rgb({\",\".join(map(str, colors[surah]))})'\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=embeddings_3d[data.index, 0],\n",
    "            y=embeddings_3d[data.index, 1],\n",
    "            z=embeddings_3d[data.index, 2],\n",
    "            text=data['text_ar'],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=10,\n",
    "                color=color,\n",
    "                opacity=0.8,\n",
    "            ),\n",
    "            name=surah_names[f'{surah}']\n",
    "        ))\n",
    "\n",
    "    # Set plot layout\n",
    "    fig.update_layout(\n",
    "        title='Distribution of Quran Verses',\n",
    "        scene=dict(\n",
    "            xaxis_title='t-SNE Dimension 1',\n",
    "            yaxis_title='t-SNE Dimension 2',\n",
    "            zaxis_title='t-SNE Dimension 3'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Export plot as HTML file\n",
    "    fig.write_html(f'{json_file}.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quran_bert_embeddings(quran_data: list):\n",
    "    # Load pre-trained BERT model and tokenizer\n",
    "    model_name = 'bert-base-uncased'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    BERT_embeddings = []\n",
    "    for verse in quran_data:\n",
    "        surah = verse['surah']\n",
    "        ayah = verse['ayah']\n",
    "        text_en = verse['text_en']\n",
    "        text_ar = verse['text_ar']\n",
    "        encoding = tokenizer(text_en, return_tensors='pt', padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            output = model(**encoding)\n",
    "            embedding = output.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "            BERT_embeddings.append(\n",
    "                {\n",
    "                    'surah': int(surah),\n",
    "                    'ayah': int(ayah),\n",
    "                    'text_en': text_en,\n",
    "                    'text_ar': text_ar,\n",
    "                    'embedding': embedding.tolist() # Convert embedding numpy array to list for JSON serialization\n",
    "                })\n",
    "            logging.info(f\"Extracted BERT embedding for surah {surah}, ayah {ayah}\")\n",
    "    return BERT_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_embeddings = quran_bert_embeddings(quran_data)\n",
    "export_embeddings_to_json(BERT_embeddings, 'data/quran_BERT_embeddings.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_embeddings_from_json('./data/quran_BERT_embeddings.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt3_embedding(content: str, engine: str='text-embedding-ada-002') -> list:\n",
    "    try:\n",
    "        response = openai.Embedding.create(input=content, engine=engine)\n",
    "        vector = response['data'][0]['embedding']\n",
    "        return vector\n",
    "    except Exception as e:\n",
    "        logging.error(f'Embedding failed. Error message: {e}')\n",
    "\n",
    "def extract_embedding(text: str) -> list:\n",
    "    try:\n",
    "        embedding = gpt3_embedding(text)\n",
    "    except:\n",
    "        while(True):\n",
    "            try:\n",
    "                if len(text) > 8191:\n",
    "                    logging.warning('[OPENAI ERROR] Trying to get shorter input < 8191 for text...')\n",
    "                    embedding = gpt3_embedding(text[:8191])\n",
    "                else:\n",
    "                    embedding = gpt3_embedding(text)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                logging.error(f'Trying to get the embedding for text. Error message: {e}')\n",
    "                time.sleep(5)\n",
    "    return embedding\n",
    "\n",
    "def quran_gpt_embedding(quran_data: list) -> list:\n",
    "    # Encode the text using the tokenizer and extract embeddings using the model\n",
    "    GPT_embeddings = []\n",
    "    for i, verse in enumerate(quran_data):\n",
    "        surah = verse['surah']\n",
    "        ayah = verse['ayah']\n",
    "        text_en = verse['text_en']\n",
    "        text_ar = verse['text_ar']\n",
    "        logging.info(f'Processing verse {i+1}: Surah {surah}, Ayah {ayah}')\n",
    "        embedding = extract_embedding(text_en)\n",
    "        GPT_embeddings.append(\n",
    "        {\n",
    "            'surah': int(surah),\n",
    "            'ayah': int(ayah),\n",
    "            'text_en': text_en,\n",
    "            'text_ar': text_ar,\n",
    "            'embedding': embedding # Convert embedding numpy array to list for JSON serialization\n",
    "        })\n",
    "        time.sleep(1)\n",
    "    return GPT_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_embeddings = quran_gpt_embedding(quran_data)\n",
    "export_embeddings_to_json(GPT_embeddings, 'data/quran_GPT_embeddings.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_embeddings_from_json('./data/quran_GPT_embeddings_.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
